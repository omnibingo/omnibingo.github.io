---
title: 聊明白机器学习 | 四. 梯度下降（Gradient Descent）　       
date: 2019-07-23 20:20:24
tags: Basic
toc: true
thumbnail: http://cdn.1ouo1.com/nwgoe.jpg
meta: true
mathjax: true
---

## 前言干话

今年春天开车到千岛湖（没骑摩托真的特别令人后悔），被一个宽阔路面的限速 40km/h 探头给逮到了...开了个 60 多，扣 12 分 🙂。 地方财政特色创收，作为没有申诉通道的小老百姓我也不好多说什么，提醒各位往淳安走的，都稍微留心一下。总之啥都不能开的日子真的很煎熬，但终于把驾照的账还清啦。

这个月还有些挠心事处理掉，就可以彻底活过来了！废话不多说，今天啃个硬骨头吧，详细聊聊梯度下降（Gradient Descent）。

<!-- more -->

## 回顾一下

之前在[回归（Regression）](https://talk2.it/post/regression)那一篇里已经涉及到了梯度下降（Gradient Descent）怎么做。即进行到了等同于[「把大象放进冰箱」](/post/introduction#寻找答案三步走)的第二步，我们确定了一个来衡量参数们「到底好不好」的损失函数（Loss Function），我们把我们的参数（可能有多个）都笼统看做一个 $\theta$ 。

那么接下来的任务进入[「把大象放进冰箱」](/post/introduction#寻找答案三步走)的第三步，就是我们要找合适的参数，让损失函数 $L(\theta)$ 值最小了。

具体做法如下，我们先随机选一组初始值（在图里面的 $\theta_y^x$ 表示在第 x 组里的第 y 个参数，需要注意）。

![](http://cdn.1ouo1.com/em9xj.png)

每次算参数们对于 $L(\theta)$ 的偏微分，然后乘上一个常数（即学习速率，Learning Rate，相当于步长，后面会再涉及到），算数取个负数（回忆一下，即原来说过的方向上反着走），与当前位置点相加，则得到下一步的位置。

如果像上图使用橙黄色部分来代入，则可以简化成上图右下的式子。橙黄色的这个 $\nabla{L(\theta)}$ 就是「梯度（Gradient）」。

找寻最佳参数的过程视觉化来看就如下图。

![](http://cdn.1ouo1.com/vae1r.png)

梯度其实就是沿着损失函数等高线的法线方向的一个量。截止到这里，就是我们原来说过的梯度下降相关的内容。

接下来我们开始讲实战里面的一些技巧。

## 三种实战技巧
### 小心调整学习速率（Learning Rate） 

调整学习速率（Learning Rate）要很小心，如下图。它控制了你每一次更新参数的步伐。太小了，步子太慢你可能等不起；太大了，又很容易直接跳过最优解。

![](http://cdn.1ouo1.com/6g1de.png)

每次做梯度下降计算时候，都留意一下上图右边这个图是很有必要的（当然了你肯定无法像现在的上帝视角一样总览全局，但），至少需要在确定我们的参数更新过程中（前几次），损失函数都能稳定下降，我们才可以考虑做别的事去，挂机让它跑。

学习速率应该怎么调整呢？很直观地感受是，如果它能动态调整就最好了：我们目标是「让损失函数坠入谷底」嘛，那一开始坡陡的时候，步子就迈大一点，后面再慢慢变小咯。并且活儿更细一些，我们每个参数如果都能精细使用不同的学习速率就更好了。

办法有的。执行这种思想，最简单的做法叫 Adagrad（这个好像真没中文名，但执行这类思想的方法名通常都以 Ada- 开头，如 Adam、Adadelta 等）。

![](http://cdn.1ouo1.com/k4law.png)

Adagrad 的操作跟均方根（Root mean square）有关，如上图。注意上标 t 意思是某一个时刻的对应值（因为很多项目在更新参数后，就又变了嘛）。Adagrad 在学习速率 $\eta$ 常量下除以了一个 $\sigma^t$，这一项代表过去所有微分值的均方根。

实际拿下图的例子看可能会更清楚，其实就是代入「过去所有微分值的均方根」来算。

![](http://cdn.1ouo1.com/i7btr.png)

进一步，根据 Adagrad 的定义（$\eta^t$的式子也是 Adagrad 里的特殊定义哦，不用困惑），可以把式子再简化成下图。

![](http://cdn.1ouo1.com/pjnrq.png)

特别注意一下 Adagrad 的参数按照它的定义这么搞，到后面正常情况下就会越更新越慢（甚至慢到令人发指），这事因为有跟 t 相关的项目，数学上就会是这个效果...这也是 Adagrad 作为 Ada 家族最简单的方法自身的短板。他的大哥 Adam、Adadelta 就不会有这个问题，这里先不展开。

当然，除此之外，对算式敏感的你有可能会发现，这个式子里的分母似乎有些问题，它会产生：梯度（图里的 $g^t$）越大，反而步长会越小的效果？

事实上，Adagrad 的一个精神是，要去参照过去的梯度都有多大，然后来体现当前的梯度与之前梯度的反差。你看最终的式子里包含 $g^i$ ，它就是过去的梯度值，和当前的梯度在式子里就可以相处有一个相对的对照效果（比如：当原来大家梯度都很小的时候，我只要相对大一些，那就能凸显出我此时的梯度其实相对很大，步子要迈大）。

![](http://cdn.1ouo1.com/b7tvz.png)

另外这个式子也说明了：每次的步长要正比于当前的微分会比较好。

这个事情还可以通过下面我们初中学的二次函数图来证明。

![](http://cdn.1ouo1.com/gjx4x.png)

我们假设要在这个图里找 $y$ 值最低点，那么我们就从随机取的 $x_0$ 开始找呗。我们以 $y$ 对 $x$ 微分，得到 $\left|2ax_0 + b\right|$ 这一项，它就是 $x_0$ 这一点的微分。而初中知识教会我们，如果我们想一步就找到最低点（$x = -2a/b$）的话，只需要在 $x_0$ 这儿跨出 $\left|x_0+b/2a\right|$ 就行。把它式子整理一下如图绿色箭头，定睛一看，这不就是上面的 $x_0$ 处的微分吗？

所以这从数学上也就说明了：每次更新的步伐，最好是跟当前微分的大小成正比（即微分越大，步子得越大哦）。

当然，上面这个二次方程还是太简单了点，就 $x$ 一个参数，那如果有多个参数的话，情况就像下图了。

![](http://cdn.1ouo1.com/33j6e.png)

别管图里的鸟语，大概意思就是我们分别去看参数 $w_1$ 和 $w_2$ 的更新过程，两个参数的下降的陡峭程度不一样，所以就可能存在「我比你陡，但是我离最低点其实更近」的现象。

但其实也可以从数学上解释和解决：注意看下图的 $2a$，我们依然是追求 $\left|x_0+b/2a\right|$ 这个完美步长，留心整理后红框框出来的分母 $2a$，这个 $2a$ 其实就是 $x$ 的二次微分值（有点数学家生搬硬套的牵强在，但没毛病...）！
 
![](http://cdn.1ouo1.com/qwgod.png)

所以就可以进一步得出：当你综合考虑了二次微分值对于步长的影响，那么你就可以跨参数来做微分值的比较了。多参数情况下，加入二次微分值，才能真正（跨参数）反应当前点与最低点的距离。

但实际操作中，二次微分算算还是很耗费资源的（当然你资源多愿意花时间来硬算，也没问题的）。所以贴心的 Adagrad 里，能做到在不增加运算负担的情况下，来通过一次微分预估二次微分值。

下图中的分母式子就可以预估二次微分的值的大小了。我们取一个二次微分值比较小的图（蓝紫色）和二次微分值比较大的图（绿色）。本来分母这个式子的意思是：把各个一次微分的值都拿来平方后开根号。但这个分母计算后得到的值，就具备了比较二次微分值的效果了！你比如都取 5 个点，来比较蓝紫图和绿图的二次微分值。显然篮紫图的二次微分值要更小（坡度更缓），同时对应式子中的分母也是蓝紫图的值更小。

![](http://cdn.1ouo1.com/k1k48.png)

### 使用随机梯度下降

随机梯度下降（Stochastic Gradient Descent）可以让训练效率再有所提高。它的精神是，每次只算一个特定样本点的损失函数和梯度，即每次看见一个样本数据就更新一次参数。

以回归问题举例，一般的损失函数定义会想下图上半部分，这很通俗，它考虑了所有样本，然后就可以做梯度下降了。

![](http://cdn.1ouo1.com/2gppn.png)

但随机梯度下降的想法就不一样，它每次只拿一个 $x_n$ 出来，所以看图上下半部分的式子就没有 $\sum$ 符号了。它不去算所有数据的梯度下降之和，而是取一个样本就更新一次，像下图的例子，同样取 20 个样本，左边常规更新一次参数，右边就可以更新 20 次。你知道天下武功唯快不破，当你出 1 拳时候我能出 20 拳，这效果当然...

![](http://cdn.1ouo1.com/ehv78.png)

### 做特征缩放

要说特征缩放（Feature Scaling）的意义，可以直接举例如下。

![](http://cdn.1ouo1.com/5xs12.png)

图里左边的 $w_1$ 对应的 $x_1$ 如果值都比较小，那么 $w_1$ 的变化对于 $y$ 的影响就比较小，这样 $w_1$ 的方向曲线就会比较平滑，反之由于 $x_2$ 都那么老大，那么 $w_2$ 的方向曲线就会比较陡峭。

特别说明一下，对于这种椭圆图的情况，它一定要求咱要有灵活变化的学习速率（比如使用 Ada 家族方法），不然是搞不定的。

但这一切在特征缩放后就会方便很多了，缩放过后，图就变圆了，从任意一点开始，直接往圆心走就对了，效率更高。而如果是椭圆，那走到一半还得拐弯...

至于缩放的方法就很多了。比如用不同组的同一位置的数据，取算术平均和标准差之后，来更新数据。使得处理之后他们新值的算数平均是 0，标准差是 1。

![](http://cdn.1ouo1.com/50xvm.png)

## 梯度下降的限制

最后我们说说梯度下降的局限性吧。首先抛出个问题：每次更新参数损失函数一定是会减小吗？

你瞧这是人话吗？前面说了那么多，当然不是啊。你比如学习速率步子扯大一点，就可能导致更新后损失函数不减小。

![](http://cdn.1ouo1.com/93n5z.png)

好，再另外就是梯度下降时，你可能会遇到微分值十分接近 0 的状况，这可能是我们之前认知的局部最小（Local Minimum）位置。但也有可能更不幸的是你其实停在了一个高原缓坡上，离所谓的最小还很远，甚至局部最小在哪你都是一无所知的...

另外现实生活中，做梯度下降就很可能遇到类似于下面两个场景的困局。

![](http://cdn.1ouo1.com/yspng.png)

比如你玩帝国时代。如果你不作弊，想让你的车子（代表你的参数组）去不断在很可怜的可视范围内踩局部最低点，你只能慢慢摸。但除非你用秘籍开个天眼，否则你几乎是不可能知道全局最小（Global Minimum）值在哪里...

![](http://cdn.1ouo1.com/9jvnh.png)

再比如你玩 Minecraft，我们老老实实用梯度下降来更新参数，但损失函数甚至有可能不降反增...比如图里这时候你看右边也是低的，前面也是低的。好，这时候往右前方跨出一步，你的位置就反倒抬升了...

所以啊，梯度下降虽然存在不少限制，但他仍然不失为找寻函数极小值的一个广泛使用的方法。后面我们还会涉及，今儿关于梯度下降咱先到这。下面我们该开始说「分类（Classification）模型」了。