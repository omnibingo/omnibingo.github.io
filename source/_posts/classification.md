---
title: 聊明白机器学习 | 五. 分类（Classification）　       
date: 2019-08-10 20:20:24
tags: Basic
toc: true
thumbnail: http://cdn.1ouo1.com/qh00v.jpg
meta: true
mathjax: true
---

## 前言干话

前几天骑摩托跑了一趟安徽小长途，皖南之山风光怡人，道不宽弯不缓但好在路面整洁鲜有砂石，人车稀少，即便穿过村庄的羊肠小道都被收拾得清清爽爽，一不留神就能满胎。特别是黄山附近的 S103、G205，黄昏和夜间骑行你甚至会恍惚从风中听到应景的 <豆腐宅急便>...

“没上黄山。”
“啊？你都到黄山脚下了没上山去？”
“每天舒舒服服睡到中午，骑个车多美，爬山下回吧...”

回杭第一天就发现有一个同样做李宏毅老师机器学习内容的团队火了。大概团队是 8 个同学用了半年时间，把李老师的课程图文版和相关代码都做了整理呈现。这个团队叫 DataWhale，该项目的[地址在这。](https://github.com/datawhalechina/leeml-notes)

突然好酸啊。虽然动得比人早，但人家已经完结了并且收获了不错的影响力，十分羡慕，团队的力量是强大。

不过相对于 DataWhale 100% 完整还原课程所有内容，本站的方向还是坚持用大白话把这事儿「聊明白」，力求让你承受尽可能小的「时间负担」和「认知负担」来搞懂机器学习。所以我还是会坚持把这块内容挤完的...啊哈哈哈哈。

当然还有个插曲，我很荣幸和意外地被拉入了上述这个 12 人的开源项目小组，希望后续有机会能一起做些有趣的事情咯。

今天废话有点多，马上开始分类（Classification）吧。

<!-- more -->

## 分类是什么有啥用

相对于回归（Regression）模型最终是通过最佳方法输出的是一个数值，我们的分类（Classification）模型，简单讲就是通过找到的最佳方法，最终输出「我们对应的输入是属于哪个类别」的这么个事。

回归模型能做的事显然有很多：

比如输入一个人的各种财务信息（收入、存款、职业、年龄、金融历史等）来判断可不可以给他批贷款；

比如输入一个患者的各种生理信息（症状、年龄、性别、用药历史等）来判断他得了哪种疾病；

比如输入一个手写汉字的图片来判断这写的是哪一个字（从 8000 多个汉字选项中找正解的分类问题）；

再比如输入一个人脸的图片来判断这是哪一个人（从海量人脸数据库中找正解的分类问题）。

![](http://cdn.1ouo1.com/e71qu.png)

## 以精灵宝可梦举例
### 分类可有用了

当然，按照惯例，我们还得扯上精灵宝可梦当例子。分类这事可以帮我们在面对一个未曾见过的宝可梦时，判断它属于那种属性。

![](http://cdn.1ouo1.com/ner32.png)

要知道宝可梦有 18 种属性，了解了对方属性后，对战时候我们就可以更有的放矢地使用克他，或防它的宝可梦了。

![](http://cdn.1ouo1.com/9lw36.png)

具体要怎么来做这个分类呢？首先我们需要将一只宝可梦数值化，即用一些特性（Feature）数值（比如总能力值、体力值、攻击力、防御力、特殊攻击力、特殊防御力、速度等）来描述一只宝可梦，比如举例皮卡丘的每一项数值如下。

![](http://cdn.1ouo1.com/5gfao.png)

等到收集到一些训练数据（宝可梦的各项特性和所属属性信息）后，就可以来训练了。

### 当成回归来处理？

有一种最简单粗暴的想法，就是把这事当做一个回归（Regression）问题来解决。

比如我们以简单的二元（Binary）分类问题为例，既然我们要用回归的办法做（输出都做数值转换），那我们在训练时候，把训练数据中的「分类 1」当做数值 1，把「分类 2」当做数值 -1。然后，我们就可以在跑测试数据时候指定，如果输出值是大于 0 的，我们就当成输出了「分类 1」，否则我们就当成输出了「分类 2」。

![](http://cdn.1ouo1.com/y6vnd.png)

好像挺合理，但又感觉不太安心对不对？

不安心就对了，因为确实会有问题，你看下面这个图。

![](http://cdn.1ouo1.com/58gmc.png)

如果事情美好得像左边这个图一样，我们就可以很自然地用「回归的办法」找到绿色的分界线，干干净净地完成分类，即这时候用「回归」的思想和「分类」的思想得出的分界线一致。

但如果此时有偏离 1 较多的一些值，比如右边那个图的右侧蓝色群，这就会开始让「回归」思想和「分类」思想出现偏差了。因为我们在回归训练时把「分类 1」都当做了数值 1 来做，在回归的思想中，它会认为需要让分布点都越接近 1 才越好，右下那些远大于 1 的点是「错误的数据」，所以如果真拿这些点来训练「回归模型」的话，分界线会被右下那些点「掰」成「紫色的那条线」。

但就「分类」模型来看，依然在右图中是绿色的分界线才最好。这因为本质上「回归」和「分类」定义一个方法好坏的方式是不一样的。

我们进一步讲，如果不是二元分类问题，而是更复杂的多元分类，此时还考虑用「回归」的方式硬解的话，比如你可能需要把「分类 1」 当成数值 1，把「分类 2」 当成数值 2；把「分类 3」 当成数值 3...

这样也是不科学的，因为此时你已经不小心悄悄引入了「分类 1」和「分类 2」有相近关系（数 1 和 2），「分类 2」和「分类 3」有相近关系这样的本不存在的逻辑干扰了。

### 合理的方案

理想的替代方案应该是下面这种，我们还拿二元分类来举例如下图。我们会在方法（function）里再内建一个 $g(x)$，然后安心用这个 $g(x)$ 的大于 0 或 小于 0 来做归类。接着对于这整个方法，它的损失方程（Loss Function）则可以定义成所有样本里结果不正确的「次数总和」。即图里面的 $δ$ 在错误是值是 1，正确时值是 0。

![](http://cdn.1ouo1.com/k1p58.png)

但定睛一看，上面这个问题的解法似乎暂时已经超出了我们的认知范围，因为这玩意没法微分啊，这就意味着无法用梯度下降（Gradient Descent）来解了。它得用到后续我们会讲的感知机（Perceptron）或者支持向量机（SVM）的知识了，但今天我们将尝试先用另外一个解法来做。

### 经典概率论解法

我们回到曾经中学的概率论课堂，假设一个蓝色球，它是从盒子 1（Box 1）中拿出来的概率，可以用下图中的式子来表达。

![](http://cdn.1ouo1.com/oht69.png)

对应到我们解决「分类」的问题里，就可以看做是我们要拿一个样本进行这两种分类，只需要知道下面红框中的四个概率值即可。

![](http://cdn.1ouo1.com/m69eb.png)

这四个值就需要我们通过训练数据训练估测出来。特别地，我们当拿到了这些数值之后，拥有这个模型，就可以自己生产 $x$ 值作为样本了，所以这种做法也被叫做「生成模型（Generative Model）」

图中的上面两个值 $P(C_1)$ 和 $P(C_2)$ 是好算的，它们也被叫做 Prior（优先要做的）。如果我们宝可梦的问题先知考虑做两种属性的分类：「水系」和「一般系」，那么两个数据就可以如下图简单算出，这很直觉不多讲。

![](http://cdn.1ouo1.com/zhdhr.png)

进入第三个值，$P(x|C_1)$ 在宝可梦中对应的解释就可以是：从「水系」宝可梦中挑（取样）一只出来，如果正好是[「原盖海龟」](https://cn.portal-pokemon.com/play/pokedex/564)的概率是多少。我们假定宝可梦们都是从一个正态分布（也叫高斯分布，Gaussian Distribution）中生成取样而来。那么现在问题就是我们怎么样通过已有的训练数据（图中拿了 79 只宝可梦数据来训练），去找到生成它们的那个高斯分布模型了。

![](http://cdn.1ouo1.com/99bmz.png)

### 找高斯分布模型

我们知道，在分类模型里每一只宝可梦都由它的一堆特征（Feature）来表示（这些特征会组成一个向量参与计算）。我们简单起见先拿「防御力」和「特殊防御力」这两个特征来作图如下。

![](http://cdn.1ouo1.com/7cl2k.png)

如果宝可梦的生成按照假设遵循一个正态分布，那么我们直接就可以拿数学上正态分布的式子如下（如果不了解可以放心不用管，就理解为它是一个方法即可）。

![](http://cdn.1ouo1.com/vj5uy.png)

这个方法输入的 $x$ 是某个宝可梦的一系列特征（向量），输出则是该宝可梦从正态分布中生成出来的概率（严格来讲是概率密度 Probability Density，它正比于概率，但这里可以简单理解为宝可梦生成出来的概率）。从数学知识中我们知道结论是这个方法的形态主要受两个因素来决定：一个是平均数 $μ$，另一个是协方差矩阵 $\Sigma$。下面两个图可以具体体现两者如何影响方法的形态变化。

![](http://cdn.1ouo1.com/dq3fr.png)
![](http://cdn.1ouo1.com/7u097.png)

图中我们可以看到不同的 $μ$ 会影响最大概率分布所在的位置，而不同的 $\Sigma$ 则会影响分布散开的程度。

我们把上面提到的 79 只训练用宝可梦放到下面的式子中。那么要解那一行式子，本质就变成了要找 $μ$ 和 $\Sigma$。

![](http://cdn.1ouo1.com/i9uf8.png)

### 极大似然估计法

我们讲用极大似然估计（Maximum Likelihood）的办法来找到样本数据对应的 $μ$ 和 $\Sigma$。它的精神大概是如下这样。

其实理论上任何一个 $μ$ 和 $\Sigma$ 所决定的正态分布都是有可能生成这 79 个训练数据的，只不过它们生成这 79 个点的概率有大有小而已。

那么我们不妨定义对于 $μ$ 和 $\Sigma$，它们来生成这 79 个点的概率表示为 $L(μ,\Sigma)$。因为每一次生成单个训练数据的概率都是相互独立的，所以这个 $L(μ,\Sigma)$ 就等于：第一个点被生成的概率 * 第二个点被生成的概率 * ... * 第七十九个点被生成的概率。

![](http://cdn.1ouo1.com/phhlo.png)

好，那现在我们就是要找出一个高斯分布，它的 $L(μ,\Sigma)$，即生成出这 79 个点的概率是最大的。

![](http://cdn.1ouo1.com/rtnq9.png)

在数学上解这个问题的代数运算并不复杂，可以通过用微分找极值，也可以通过背公式解决...总之我们可以容易地在数学上找到对应概率最大的高斯分布（和对应的 $μ$ 和 $\Sigma$）。

所以对于这 79 个水系宝可梦的训练数据，我们得到最大概率生成它们的高斯分布的 $μ^1$ 和 $\Sigma^1$，如图。

特别地，我们还有 61 个一般系的宝可梦，我们也通过一样的方法找到最大概率生成它们的高斯分布的 $μ^2$ 和 $\Sigma^2$。

![](http://cdn.1ouo1.com/l7fgr.png)

那既然得到了两个分类的高斯分布，概率论式子里所需要的值我们都可以算啦，赶紧带进来试试。

![](http://cdn.1ouo1.com/tkl75.png)

然而我们发现，对于训练集来说，两个类别的分界线不是特别明显，并且在测试数据上跑起来效果也不怎么满意，只有 47% 的准确率。

有没有可能是引入的维度不够？人家本来有 7 个特性，7 维空间里，可能分界线就很明显了呢！然而并没有，7 个维度都带进来了，最终算得的测试集准确率也只有 54%。

还有没有进一步优化的办法呢？有。

### 进一步优化

我们可以让不同的分类来共用同一个协方差矩阵（Covariance Matrix，就是那个 $\Sigma$），这个协方差矩阵是跟输入的特性个数的平方成正比的，如果每一个分类都用不同的协方差矩阵，那会导致模型的参数变得过多，由此可萌导致方差（Variance）过大，最终过度拟合（Overfitting），这个是前序的知识，很好理解。

于是我们考虑共用协方差矩阵来减少参数。

![](http://cdn.1ouo1.com/4pcau.png)

共用了 $\Sigma$ 之后，寻找极大似然的式子如下图。注意此时生成所有样本数据（79 个水系加上 61 个一般系）的概率变成了：用 $μ^1$ 和 $\Sigma$ 生成每个水系宝可梦的概率相乘，再乘以用 $μ^2$ 和 $\Sigma$ 生成的每个一般系宝可梦的概率。

![](http://cdn.1ouo1.com/0mmxf.png)

上图中 $μ^1$ 和 $μ^2$ 还是通过老办法，分别把分类 1 和分类 2 中所有 $x$ 算数平均得到。 $\Sigma$ 同时考虑两个类别，所以兼顾一下两边分别生成样本数量的加权权重，这也好理解。

经过一顿算，我们发现如果共用协方差矩阵（$\Sigma$）后，两个分类的分界线变成了直线。所以虽然高斯分布不是线性的，但这种俩分类分界线是直线的，我们也叫这个模型为「线性模型（Linear Model）」，当然，用了不同 $\Sigma$ 就不是线性模型了。

![](http://cdn.1ouo1.com/o86k1.png)

进一步，我们用上所有宝可梦的特性参数作为维度，我们可以把正确率提高到 73% 了。

### 概率论方法总结

我们来总结一下。

实际上上述解决分类问题还是那老三步。

第一步，选择模型。我们这一次确定了用中学概率论的那个式子做模型，那么就牵扯到我们得去找 $P(C_1)$、$P(C_2)$、$P(x|C_1)$、$P(x|C_2)$、这些数值的不同就构成了不同的方法。

第二步，定义好坏。即定义一个方法产生我们的训练数据集的概率有多大，这个概率就是它的「好坏程度」。

第三步，找出它。找出那个产生训练数据集概率最大的方法（对应的 $μ$ 和 $\Sigma$），这个过程上面也都接触到了，都是些常规的数学处理，明白道理即可。

![](http://cdn.1ouo1.com/401ir.png)

当然话说回来，解决这个分类问题，除了上面一直用的高斯分布模型之外，还有很多模型可以供你选择，具体选什么模型那这就是看你人的功夫了。无非就是需要注意那条不变的道理：模型越简单，偏离值（Bias）会越大，方差（Variance）会越小，反之亦然。

还有一种情况，如果对于一个样本它的每个特征值（$x$）生成的概率都是完全独立的（显然宝可梦不是这样的例子，比如攻击力跟防御力实际很可能是正相关的），实际上就没有必要再用高斯分布模型来做了。比如如果这个特性是二元的（0 或 1 两种状态）则可以直接使用伯努利分布（Bernoulli Distribution，也叫两点分布、0-1 分布）来做。

![](http://cdn.1ouo1.com/6uxjt.png)

而如果真的所有特性都是独立产生的，即我们不需要去理会每个特性间的协方差（Covariance）关系，我们可以直接使用朴素贝叶斯分类器（Naive Bayes Classifier）来做。这个分类器在我们的这个假设（特性之间都是独立无关的）成立的时候，表现还是很棒的。当然，如果假设不成立，则它的偏离值（Bias）会很很大。

### 问题竟还能精简和转化

我们再回头来研究一下我们一开始想求的这个概率论式子 $P(C_1|x)$，我们其实可以对它做如下图的转化，最终可以得到一个关于参数 $z$ 的 S 型生长曲线函数（Sigmoid Function）。这个函数当 $z$ 趋近于无穷大时，函数值趋近于 1，当 $z$ 趋近于负无穷时，函数值趋近于 0。

![](http://cdn.1ouo1.com/c6ha0.png)

其实在数学上我们接着可以对 $z$ 做一顿计算，最终在共用 $\Sigma$ 的情况下，简化出一个 这个概率最终等于一个关于 $wx+b$ 的式子。这其实也就解释了为啥我们之前的两个分类分界线是直线。

![](http://cdn.1ouo1.com/9tp9i.png)

那话说回来了，我们最终忙活半天又是 $N$（分类样本数量）又是 $μ$ 又是 $\Sigma$ 的，我们为什么不直接去找 $w$ 和 $b$ 呢？

这就是我们接下来要说的「逻辑回归（Logistic Regression）」问题了，今天先到这儿，晚安。








