{"title":"聊明白机器学习 | 四. 梯度下降（Gradient Descent）　","date":"2019-07-23T12:20:24.000Z","thumbnail":"http://cdn.1ouo1.com/nwgoe.jpg","slug":"gradient_descent","comments":true,"tags":["Basic"],"updated":"2019-08-14T11:35:04.000Z","content":"<h2 id=\"前言干话\"><a class=\"markdownIt-Anchor\" href=\"#前言干话\"></a> 前言干话<a href=\"post/gradient_descent#前言干话\"></a></h2>\n<p>今年春天开车到千岛湖（没骑摩托真的特别令人后悔），被一个宽阔路面的限速 40km/h 探头给逮到了…开了个 60 多，扣 12 分 🙂。 地方财政特色创收，作为没有申诉通道的小老百姓我也不好多说什么，提醒各位往淳安走的，都稍微留心一下。总之啥都不能开的日子真的很煎熬，但终于把驾照的账还清啦。</p>\n<p>这个月还有些挠心事处理掉，就可以彻底活过来了！废话不多说，今天啃个硬骨头吧，详细聊聊梯度下降（Gradient Descent）。</p>\n<a id=\"more\"></a>\n<h2 id=\"回顾一下\"><a class=\"markdownIt-Anchor\" href=\"#回顾一下\"></a> 回顾一下<a href=\"post/gradient_descent#回顾一下\"></a></h2>\n<p>之前在<a href=\"https://talk2.it/post/regression\">回归（Regression）</a>那一篇里已经涉及到了梯度下降（Gradient Descent）怎么做。即进行到了等同于<a href=\"/post/introduction#%E5%AF%BB%E6%89%BE%E7%AD%94%E6%A1%88%E4%B8%89%E6%AD%A5%E8%B5%B0\">「把大象放进冰箱」</a>的第二步，我们确定了一个来衡量参数们「到底好不好」的损失函数（Loss Function），我们把我们的参数（可能有多个）都笼统看做一个 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 。</p>\n<p>那么接下来的任务进入<a href=\"/post/introduction#%E5%AF%BB%E6%89%BE%E7%AD%94%E6%A1%88%E4%B8%89%E6%AD%A5%E8%B5%B0\">「把大象放进冰箱」</a>的第三步，就是我们要找合适的参数，让损失函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>L</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">L(\\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span> 值最小了。</p>\n<p>具体做法如下，我们先随机选一组初始值（在图里面的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>θ</mi><mi>y</mi><mi>x</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\theta_y^x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.077548em;vertical-align:-0.383108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-2.4530000000000003em;margin-left:-0.02778em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">x</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.383108em;\"><span></span></span></span></span></span></span></span></span></span> 表示在第 x 组里的第 y 个参数，需要注意）。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/em9xj.png\" alt data-zoomable></p></div>\n<p>每次算参数们对于 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>L</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">L(\\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span> 的偏微分，然后乘上一个常数（即学习速率，Learning Rate，相当于步长，后面会再涉及到），算数取个负数（回忆一下，即原来说过的方向上反着走），与当前位置点相加，则得到下一步的位置。</p>\n<p>如果像上图使用橙黄色部分来代入，则可以简化成上图右下的式子。橙黄色的这个 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">∇</mi><mrow><mi>L</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\nabla{L(\\theta)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">∇</span><span class=\"mord\"><span class=\"mord mathdefault\">L</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span></span> 就是「梯度（Gradient）」。</p>\n<p>找寻最佳参数的过程视觉化来看就如下图。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/vae1r.png\" alt data-zoomable></p></div>\n<p>梯度其实就是沿着损失函数等高线的法线方向的一个量。截止到这里，就是我们原来说过的梯度下降相关的内容。</p>\n<p>接下来我们开始讲实战里面的一些技巧。</p>\n<h2 id=\"三种实战技巧\"><a class=\"markdownIt-Anchor\" href=\"#三种实战技巧\"></a> 三种实战技巧<a href=\"post/gradient_descent#三种实战技巧\"></a></h2>\n<h3 id=\"小心调整学习速率learning-rate\"><a class=\"markdownIt-Anchor\" href=\"#小心调整学习速率learning-rate\"></a> 小心调整学习速率（Learning Rate）<a href=\"post/gradient_descent#小心调整学习速率learning-rate\"></a></h3>\n<p>调整学习速率（Learning Rate）要很小心，如下图。它控制了你每一次更新参数的步伐。太小了，步子太慢你可能等不起；太大了，又很容易直接跳过最优解。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/6g1de.png\" alt data-zoomable></p></div>\n<p>每次做梯度下降计算时候，都留意一下上图右边这个图是很有必要的（当然了你肯定无法像现在的上帝视角一样总览全局，但），至少需要在确定我们的参数更新过程中（前几次），损失函数都能稳定下降，我们才可以考虑做别的事去，挂机让它跑。</p>\n<p>学习速率应该怎么调整呢？很直观地感受是，如果它能动态调整就最好了：我们目标是「让损失函数坠入谷底」嘛，那一开始坡陡的时候，步子就迈大一点，后面再慢慢变小咯。并且活儿更细一些，我们每个参数如果都能精细使用不同的学习速率就更好了。</p>\n<p>办法有的。执行这种思想，最简单的做法叫 Adagrad（这个好像真没中文名，但执行这类思想的方法名通常都以 Ada- 开头，如 Adam、Adadelta 等）。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/k4law.png\" alt data-zoomable></p></div>\n<p>Adagrad 的操作跟均方根（Root mean square）有关，如上图。注意上标 t 意思是某一个时刻的对应值（因为很多项目在更新参数后，就又变了嘛）。Adagrad 在学习速率 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>η</mi></mrow><annotation encoding=\"application/x-tex\">\\eta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span></span></span></span> 常量下除以了一个 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>σ</mi><mi>t</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\sigma^t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7935559999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7935559999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span></span></span></span></span></span></span></span>，这一项代表过去所有微分值的均方根。</p>\n<p>实际拿下图的例子看可能会更清楚，其实就是代入「过去所有微分值的均方根」来算。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/i7btr.png\" alt data-zoomable></p></div>\n<p>进一步，根据 Adagrad 的定义（<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>η</mi><mi>t</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\eta^t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9879959999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7935559999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span></span></span></span></span></span></span></span>的式子也是 Adagrad 里的特殊定义哦，不用困惑），可以把式子再简化成下图。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/pjnrq.png\" alt data-zoomable></p></div>\n<p>特别注意一下 Adagrad 的参数按照它的定义这么搞，到后面正常情况下就会越更新越慢（甚至慢到令人发指），这事因为有跟 t 相关的项目，数学上就会是这个效果…这也是 Adagrad 作为 Ada 家族最简单的方法自身的短板。他的大哥 Adam、Adadelta 就不会有这个问题，这里先不展开。</p>\n<p>当然，除此之外，对算式敏感的你有可能会发现，这个式子里的分母似乎有些问题，它会产生：梯度（图里的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>g</mi><mi>t</mi></msup></mrow><annotation encoding=\"application/x-tex\">g^t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9879959999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7935559999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span></span></span></span></span></span></span></span>）越大，反而步长会越小的效果？</p>\n<p>事实上，Adagrad 的一个精神是，要去参照过去的梯度都有多大，然后来体现当前的梯度与之前梯度的反差。你看最终的式子里包含 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>g</mi><mi>i</mi></msup></mrow><annotation encoding=\"application/x-tex\">g^i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.019104em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.824664em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span></span></span></span></span></span></span></span> ，它就是过去的梯度值，和当前的梯度在式子里就可以相处有一个相对的对照效果（比如：当原来大家梯度都很小的时候，我只要相对大一些，那就能凸显出我此时的梯度其实相对很大，步子要迈大）。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/b7tvz.png\" alt data-zoomable></p></div>\n<p>另外这个式子也说明了：每次的步长要正比于当前的微分会比较好。</p>\n<p>这个事情还可以通过下面我们初中学的二次函数图来证明。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/gjx4x.png\" alt data-zoomable></p></div>\n<p>我们假设要在这个图里找 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> 值最低点，那么我们就从随机取的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">x_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 开始找呗。我们以 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> 对 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span></span></span></span> 微分，得到 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo fence=\"true\">∣</mo><mn>2</mn><mi>a</mi><msub><mi>x</mi><mn>0</mn></msub><mo>+</mo><mi>b</mi><mo fence=\"true\">∣</mo></mrow><annotation encoding=\"application/x-tex\">\\left|2ax_0 + b\\right|</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\">∣</span><span class=\"mord\">2</span><span class=\"mord mathdefault\">a</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathdefault\">b</span><span class=\"mclose delimcenter\" style=\"top:0em;\">∣</span></span></span></span></span> 这一项，它就是 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">x_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 这一点的微分。而初中知识教会我们，如果我们想一步就找到最低点（<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi><mo>=</mo><mo>−</mo><mn>2</mn><mi>a</mi><mi mathvariant=\"normal\">/</mi><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">x = -2a/b</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">−</span><span class=\"mord\">2</span><span class=\"mord mathdefault\">a</span><span class=\"mord\">/</span><span class=\"mord mathdefault\">b</span></span></span></span>）的话，只需要在 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">x_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 这儿跨出 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo fence=\"true\">∣</mo><msub><mi>x</mi><mn>0</mn></msub><mo>+</mo><mi>b</mi><mi mathvariant=\"normal\">/</mi><mn>2</mn><mi>a</mi><mo fence=\"true\">∣</mo></mrow><annotation encoding=\"application/x-tex\">\\left|x_0+b/2a\\right|</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\">∣</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathdefault\">b</span><span class=\"mord\">/</span><span class=\"mord\">2</span><span class=\"mord mathdefault\">a</span><span class=\"mclose delimcenter\" style=\"top:0em;\">∣</span></span></span></span></span> 就行。把它式子整理一下如图绿色箭头，定睛一看，这不就是上面的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">x_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 处的微分吗？</p>\n<p>所以这从数学上也就说明了：每次更新的步伐，最好是跟当前微分的大小成正比（即微分越大，步子得越大哦）。</p>\n<p>当然，上面这个二次方程还是太简单了点，就 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span></span></span></span> 一个参数，那如果有多个参数的话，情况就像下图了。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/33j6e.png\" alt data-zoomable></p></div>\n<p>别管图里的鸟语，大概意思就是我们分别去看参数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">w_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>w</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">w_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的更新过程，两个参数的下降的陡峭程度不一样，所以就可能存在「我比你陡，但是我离最低点其实更近」的现象。</p>\n<p>但其实也可以从数学上解释和解决：注意看下图的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>2</mn><mi>a</mi></mrow><annotation encoding=\"application/x-tex\">2a</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span><span class=\"mord mathdefault\">a</span></span></span></span>，我们依然是追求 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo fence=\"true\">∣</mo><msub><mi>x</mi><mn>0</mn></msub><mo>+</mo><mi>b</mi><mi mathvariant=\"normal\">/</mi><mn>2</mn><mi>a</mi><mo fence=\"true\">∣</mo></mrow><annotation encoding=\"application/x-tex\">\\left|x_0+b/2a\\right|</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\">∣</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathdefault\">b</span><span class=\"mord\">/</span><span class=\"mord\">2</span><span class=\"mord mathdefault\">a</span><span class=\"mclose delimcenter\" style=\"top:0em;\">∣</span></span></span></span></span> 这个完美步长，留心整理后红框框出来的分母 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>2</mn><mi>a</mi></mrow><annotation encoding=\"application/x-tex\">2a</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span><span class=\"mord mathdefault\">a</span></span></span></span>，这个 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>2</mn><mi>a</mi></mrow><annotation encoding=\"application/x-tex\">2a</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span><span class=\"mord mathdefault\">a</span></span></span></span> 其实就是 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span></span></span></span> 的二次微分值（有点数学家生搬硬套的牵强在，但没毛病…）！</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/qwgod.png\" alt data-zoomable></p></div>\n<p>所以就可以进一步得出：当你综合考虑了二次微分值对于步长的影响，那么你就可以跨参数来做微分值的比较了。多参数情况下，加入二次微分值，才能真正（跨参数）反应当前点与最低点的距离。</p>\n<p>但实际操作中，二次微分算算还是很耗费资源的（当然你资源多愿意花时间来硬算，也没问题的）。所以贴心的 Adagrad 里，能做到在不增加运算负担的情况下，来通过一次微分预估二次微分值。</p>\n<p>下图中的分母式子就可以预估二次微分的值的大小了。我们取一个二次微分值比较小的图（蓝紫色）和二次微分值比较大的图（绿色）。本来分母这个式子的意思是：把各个一次微分的值都拿来平方后开根号。但这个分母计算后得到的值，就具备了比较二次微分值的效果了！你比如都取 5 个点，来比较蓝紫图和绿图的二次微分值。显然篮紫图的二次微分值要更小（坡度更缓），同时对应式子中的分母也是蓝紫图的值更小。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/k1k48.png\" alt data-zoomable></p></div>\n<h3 id=\"使用随机梯度下降\"><a class=\"markdownIt-Anchor\" href=\"#使用随机梯度下降\"></a> 使用随机梯度下降<a href=\"post/gradient_descent#使用随机梯度下降\"></a></h3>\n<p>随机梯度下降（Stochastic Gradient Descent）可以让训练效率再有所提高。它的精神是，每次只算一个特定样本点的损失函数和梯度，即每次看见一个样本数据就更新一次参数。</p>\n<p>以回归问题举例，一般的损失函数定义会想下图上半部分，这很通俗，它考虑了所有样本，然后就可以做梯度下降了。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/2gppn.png\" alt data-zoomable></p></div>\n<p>但随机梯度下降的想法就不一样，它每次只拿一个 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 出来，所以看图上下半部分的式子就没有 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>∑</mo></mrow><annotation encoding=\"application/x-tex\">\\sum</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.00001em;vertical-align:-0.25001em;\"></span><span class=\"mop op-symbol small-op\" style=\"position:relative;top:-0.0000050000000000050004em;\">∑</span></span></span></span> 符号了。它不去算所有数据的梯度下降之和，而是取一个样本就更新一次，像下图的例子，同样取 20 个样本，左边常规更新一次参数，右边就可以更新 20 次。你知道天下武功唯快不破，当你出 1 拳时候我能出 20 拳，这效果当然…</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/ehv78.png\" alt data-zoomable></p></div>\n<h3 id=\"做特征缩放\"><a class=\"markdownIt-Anchor\" href=\"#做特征缩放\"></a> 做特征缩放<a href=\"post/gradient_descent#做特征缩放\"></a></h3>\n<p>要说特征缩放（Feature Scaling）的意义，可以直接举例如下。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/5xs12.png\" alt data-zoomable></p></div>\n<p>图里左边的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">w_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 对应的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">x_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 如果值都比较小，那么 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">w_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的变化对于 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> 的影响就比较小，这样 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">w_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的方向曲线就会比较平滑，反之由于 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">x_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 都那么老大，那么 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>w</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">w_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的方向曲线就会比较陡峭。</p>\n<p>特别说明一下，对于这种椭圆图的情况，它一定要求咱要有灵活变化的学习速率（比如使用 Ada 家族方法），不然是搞不定的。</p>\n<p>但这一切在特征缩放后就会方便很多了，缩放过后，图就变圆了，从任意一点开始，直接往圆心走就对了，效率更高。而如果是椭圆，那走到一半还得拐弯…</p>\n<p>至于缩放的方法就很多了。比如用不同组的同一位置的数据，取算术平均和标准差之后，来更新数据。使得处理之后他们新值的算数平均是 0，标准差是 1。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/50xvm.png\" alt data-zoomable></p></div>\n<h2 id=\"梯度下降的限制\"><a class=\"markdownIt-Anchor\" href=\"#梯度下降的限制\"></a> 梯度下降的限制<a href=\"post/gradient_descent#梯度下降的限制\"></a></h2>\n<p>最后我们说说梯度下降的局限性吧。首先抛出个问题：每次更新参数损失函数一定是会减小吗？</p>\n<p>你瞧这是人话吗？前面说了那么多，当然不是啊。你比如学习速率步子扯大一点，就可能导致更新后损失函数不减小。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/93n5z.png\" alt data-zoomable></p></div>\n<p>好，再另外就是梯度下降时，你可能会遇到微分值十分接近 0 的状况，这可能是我们之前认知的局部最小（Local Minimum）位置。但也有可能更不幸的是你其实停在了一个高原缓坡上，离所谓的最小还很远，甚至局部最小在哪你都是一无所知的…</p>\n<p>另外现实生活中，做梯度下降就很可能遇到类似于下面两个场景的困局。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/yspng.png\" alt data-zoomable></p></div>\n<p>比如你玩帝国时代。如果你不作弊，想让你的车子（代表你的参数组）去不断在很可怜的可视范围内踩局部最低点，你只能慢慢摸。但除非你用秘籍开个天眼，否则你几乎是不可能知道全局最小（Global Minimum）值在哪里…</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/9jvnh.png\" alt data-zoomable></p></div>\n<p>再比如你玩 Minecraft，我们老老实实用梯度下降来更新参数，但损失函数甚至有可能不降反增…比如图里这时候你看右边也是低的，前面也是低的。好，这时候往右前方跨出一步，你的位置就反倒抬升了…</p>\n<p>所以啊，梯度下降虽然存在不少限制，但他仍然不失为找寻函数极小值的一个广泛使用的方法。后面我们还会涉及，今儿关于梯度下降咱先到这。下面我们该开始说「分类（Classification）模型」了。</p>\n","prev":{"title":"聊明白机器学习 | 五. 分类（Classification）　","slug":"classification"},"next":{"title":"每日一機 | 第一季完结","slug":"machines"},"link":"https://talk2.it/post/gradient_descent/","toc":[{"title":"<a class=\"markdownIt-Anchor\"></a> 前言干话","id":"前言干话","index":"1"},{"title":"<a class=\"markdownIt-Anchor\"></a> 回顾一下","id":"回顾一下","index":"2"},{"title":"<a class=\"markdownIt-Anchor\"></a> 三种实战技巧","id":"三种实战技巧","index":"3","children":[{"title":"<a class=\"markdownIt-Anchor\"></a> 小心调整学习速率（Learning Rate）","id":"小心调整学习速率learning-rate","index":"3.1"},{"title":"<a class=\"markdownIt-Anchor\"></a> 使用随机梯度下降","id":"使用随机梯度下降","index":"3.2"},{"title":"<a class=\"markdownIt-Anchor\"></a> 做特征缩放","id":"做特征缩放","index":"3.3"}]},{"title":"<a class=\"markdownIt-Anchor\"></a> 梯度下降的限制","id":"梯度下降的限制","index":"4"}],"reward":true,"copyright":{"author":"Bingo","link":"<a href=\"https://talk2.it/post/gradient_descent/\" title=\"聊明白机器学习 | 四. 梯度下降（Gradient Descent）　\">https://talk2.it/post/gradient_descent/</a>","license":"Attribution-NonCommercial-NoDerivatives 4.0 International (<a href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\" rel=\"external nofollow noopener\" target=\"_blank\">CC BY-NC-ND 4.0</a>)"}}