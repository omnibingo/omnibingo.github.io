{"title":"聊明白机器学习 | 二. 回归（Regression）　","date":"2019-05-15T12:20:24.000Z","thumbnail":"http://cdn.1ouo1.com/32mms.jpg","slug":"regression","comments":true,"tags":["Basic"],"updated":"2019-05-23T09:30:21.000Z","content":"<h2 id=\"前言干话\"><a class=\"markdownIt-Anchor\" href=\"#前言干话\"></a> 前言干话<a href=\"post/regression#前言干话\"></a></h2>\n<p>继续更新「聊明白机器学习」系列内容。抱歉今天才更新第二篇，实际是过程中启动了另一个系列：<a href=\"https://ar.talk2.it\" target=\"_blank\" rel=\"noopener\">對話 · AR</a>的准备和整理发布超出了预期时间，增强现实（AR）的内容真比想象中多而且杂啊。和人工智能（AI）一样，相信它也同样会是下一代计算平台重要依托。目前<a href=\"https://talk2.it\">「對話 · 機器 | Talk2.it」</a>的内容也主要会围绕这两个方向展开。</p>\n<p>下面进入正文，今天我们讲第一种机器学习里遇到的问题模型：回归。</p>\n<a id=\"more\"></a>\n<h2 id=\"拿例子说回归\"><a class=\"markdownIt-Anchor\" href=\"#拿例子说回归\"></a> 拿例子说回归<a href=\"post/regression#拿例子说回归\"></a></h2>\n<p>除了我们之前一章说的根据历史空气数据，推测未来 PM2.5 之外，回归（Regression）模型的例子还有预测股市（输入各种影响指标，输出指定标的的涨跌幅）、无人驾驶（输入各种传感型号，输出方向盘转向角度）、推荐系统（输入某用户和某商品，输出匹配度或购买可能性）等等。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/g49ng.png\" alt data-zoomable></p></div>\n<p>总而言之，所谓「回归」，就是最终输出一个数值的模型。</p>\n<h3 id=\"我的宝可梦要不要进化\"><a class=\"markdownIt-Anchor\" href=\"#我的宝可梦要不要进化\"></a> 我的宝可梦要不要进化？<a href=\"post/regression#我的宝可梦要不要进化\"></a></h3>\n<p>再来具体细说一个可以应用到回归模型的例子：预测一只宝可梦（Pokemon，即原来的「宠物小精灵」或「神奇宝贝」）进化后的战斗值（CP，Combat Power）。我们知道，一只宝可梦你给他吃糖，他就会进化嘛，随之而来的就是战斗值会变。我们如果能预测宝可梦进化后的战斗值，我们就可以更好地决定要不要进化他，如果觉得划不来的话，可能就把这个宝可梦直接做糖了也说不定…</p>\n<p>我们拿一只妙蛙种子为例。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/9wti6.png\" alt data-zoomable></p></div>\n<p>要比较精准地预测进化后的战斗值，我们需要尽可能搜集全宝可梦进化前的所有相关信息作为输入（input），包括：<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mi>s</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">s</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>（物种）、<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mrow><mi>c</mi><mi>p</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">x_{cp}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.716668em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">c</span><span class=\"mord mathdefault mtight\">p</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span>（当前战斗值）、<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mrow><mi>h</mi><mi>p</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">x_{hp}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.716668em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361079999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">h</span><span class=\"mord mathdefault mtight\">p</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span>（生命值）、<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mi>w</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>（重量）、<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mi>h</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_h</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">h</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>（身高）等等，而输出（output）就是我们要的他进化后的战斗值 y。</p>\n<p>还记得「把大象放进冰箱三部曲」吗？我们现在要开始第一步了：找模型（Model）。</p>\n<h3 id=\"寻找模型\"><a class=\"markdownIt-Anchor\" href=\"#寻找模型\"></a> 寻找模型<a href=\"post/regression#寻找模型\"></a></h3>\n<p>模型其实就是一个表示最终输入输出关系的式子。看起来毫无头绪对吗？那我们就从最简单的模型找起。如果它是个数学上「一次方」的式子，并且只看进化前的战斗值（cp）特性，比如这样：<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>=</mo><mi>b</mi><mo>+</mo><mi>w</mi><mo>∗</mo><msub><mi>x</mi><mrow><mi>c</mi><mi>p</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">y = b + w * x_{cp}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.77777em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\">b</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.46528em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.716668em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">c</span><span class=\"mord mathdefault mtight\">p</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span>。注意，这里通过变化 w 和 b，我们可以得到无数个不一样式子。但具体到实例中，注意会有一些必要限制。比如下图的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>f</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">f_3</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">3</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>，结果是个负值了，这种显然是不对的，后面需要留意筛掉。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/4thus.png\" alt data-zoomable></p></div>\n<p>图里的这个式子，我们都知道，在数学上也叫「线性模型（linear Model）」。其中 x 会有好多个（带下标），它们代表了一直宝可梦的不同特性（Feature），乘以 x 的这个 w 通常叫做比重值（Weight），加上的这个 b 通常叫做偏离值（Bias）。</p>\n<p>假设我们已经确定了这个方法（Function）集合作为模型，那么第二步我们要开始定义集合里，什么是好的方法了。</p>\n<h3 id=\"定义方法好坏\"><a class=\"markdownIt-Anchor\" href=\"#定义方法好坏\"></a> 定义方法好坏<a href=\"post/regression#定义方法好坏\"></a></h3>\n<p>为了训练这个模型，我们事先已经辛辛苦苦搜集到了 10 只真实的宝可梦以它们分别进化后的战斗值数据，我们已经把它们标在图上。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/nncyw.png\" alt data-zoomable><br>\n<img src=\"http://cdn.1ouo1.com/yrloc.png\" alt data-zoomable></p></div>\n<p>定义一个方法好坏的思路很单纯，其实就是使用这个方法，当输入是我们真实训练数据的宝可梦数据时，输出的结果跟真实结果数据对比，差异越小，方法越好。</p>\n<p>这里我们用数学方法来完成这个操作。我们引入一个新函数叫做「损失函数（Loss Function）」，有时它也被简称为「L 函数」。这个 L 函数的作用就是评估一个函数有多差。即：L 函数的输入是一个函数，输出是这个输入的函数有夺不好。</p>\n<p>说到底，撇开每组数据的 x、y 都不同，其实 L 函数就是来界定一组「w 和 b 参数」的好坏的。即问题变成了：我们需要选一组最合适的 w 和 b，让 L 函数最小。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/b19eb.png\" alt data-zoomable></p></div>\n<p>带进式子里算呗。图里的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> 是真实数据的结果值，它减的这个则是当前方法（Function）的估测值。我们把两者之差取平方，就得到了一组数据的估测误差。把 10 组数据的误差都加起来，就得到了 L 函数（其实如果再除以 10，就是数学上的「方差」了，这个估测误差的道理是一样的）。</p>\n<p>显然，此时 L 函数如果越大，表明这个方法越不好。事实上我们可以瞄一眼 L 函数对于 w 和 b 作的图如下。里面的每个点都代表了一组 w 和 b，颜色越暖 L 函数数值越大，越不好。所以方向上我们需要找颜色最冷的地方对应的 w 和 b 参数。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/yj9a5.png\" alt data-zoomable></p></div>\n<p>所以接下来我们要进行最终一步了，就是开始找到这对最合适的 w 和 b。</p>\n<h3 id=\"找到最佳参数\"><a class=\"markdownIt-Anchor\" href=\"#找到最佳参数\"></a> 找到最佳参数<a href=\"post/regression#找到最佳参数\"></a></h3>\n<p>对于现在我们得到的这个式子，要解出 w 和 b，实际上「线性代数」已经教过我们怎么解了，但我相信正常人这时大概都一点印象没有吧…所以这里讲一个实操性更强的方法：梯度下降（Gradient Descent）。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/jcyxm.png\" alt data-zoomable></p></div>\n<p>梯度下降，强大就强大在他不只可以解这类的式子，事实上只要式子可微分的（Differentiable），梯度下降都可以解它。</p>\n<p>他的大概操作方法是：设定一个参数的初始值后，根据当前点位的切线斜率不断小碎步移动下降，一步步逼近最终使式子值最小的参数。</p>\n<p>我们先拿一个简单例子来说明：找一个 w 让 L 函数值最小。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/66fbc.png\" alt data-zoomable></p></div>\n<p>首先我们随机选个 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">w_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>，然后根据操作方法，做 w 对 L 函数的微分。其实就是看下切线斜率，根据斜率的正负，决定 w 往什么方向走。此时就可以把 w 想做一个左右观望的人，看哪边低就往哪边走。</p>\n<p>方向显而易见了，图里就应该往右走。那么具体一步要走多少呢？这里先大概说明白道理，后续再仔细从数学上说明。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/vuw5x.png\" alt data-zoomable></p></div>\n<p>其实一步走多少，即「步长」取决于现在的微分值（可以理解为「斜率」）有多大。道理其实也很单纯：微分越大，说明现在线越陡峭，变化越剧烈，实际离最终微分值为 0 的平缓地带就越远，那么步子就应该「扯大一点」。即：微分越大，步子越大。</p>\n<p>于是我们设置了一个手调参数 η，这个需要预算前事先凭经验定好。η 越大，更新幅度（步长）就越大。另外由于微分值和运动方向正好是相反的，所以 η 前有个负号。</p>\n<p>按照上面的变化量和变化方向，我们可以由 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">w_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 计算得到 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">w_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/dczlf.png\" alt data-zoomable></p></div>\n<p>然后再算 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">w_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的微分，移动，得到 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>w</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">w_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>，再算微分，再移动，如此反复…经过多次参数的更新，我们就会找到图中 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>w</mi><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">w_T</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.32833099999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的地方，在这里微分是 0，参数就卡住不动了。但很不幸，在这个图里，其实 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>w</mi><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">w_T</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.32833099999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 不是最优解，他只是一个局部最小值（Local minimum），但实际还有一个更右边的全局最佳解，不过我们走不过去了…</p>\n<p>不过所幸在线性回归（Linear Regression）的问题里，一般不会有局部最小值的问题，这类问题一般只会有一个最优解。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/wfb4f.png\" alt data-zoomable></p></div>\n<p>回到最初我们的问题，其实我们的式子中是有 w 和 b 两个参数的。上面例子我们简化成了只有 w 一个参数时的情况。</p>\n<p>有两个参数其实道理也是一样的。我们指定了起始点 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">w_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>b</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">b_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">b</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 后，分别算此时 w 对 L 函数的偏微分、b 对 L 函数的偏微分。然后再分别按照图中的式子来更新点位 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">w_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>b</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">b_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">b</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 到 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">w_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>b</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">b_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">b</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>，再算偏微分，再更新…如此反复，最后找到使 L 函数尽可能小的 w 和 b 值。</p>\n<p>特别注意一下，数学上在这时喜欢把 w 对 L 函数的偏微分、b 对 L 函数的偏微分排成一个向量（Vector）来表示，如上图中右上角。这一项就叫 ΔL，即梯度（Gradient）。但介于正常人应该都不喜欢看这么个玩意，后面我们会尽量少来这么表示。</p>\n<p>我们拿 w 和 b 对 L 函数作图实际应该是如下的图。还是颜色越暖代表 L 函数越大，颜色越冷代表 L 函数越小。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/8fq1b.png\" alt data-zoomable></p></div>\n<p>假设我们选取的初始值在左下角，我们按照前述开始计算方向，更新参数。其实从图上视觉化来看，每一次我们的走向实际上都应该是等高线的法线方向。</p>\n<p>其实这也就打消了之前的那个令人担忧的问题：如果以 w 和 b 对 L 函数作图如下图左边就麻烦了。因为如果起始点在左边，我们就只能走到局部最小值（Local Minimum），只有从右边出发，才能得到最优解。即能不能找到最右还得看人品…</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/6z62u.png\" alt data-zoomable></p></div>\n<p>所幸其实在线性回归（Linear Regression）的问题里，L 函数都是<a href=\"https://zh.wikipedia.org/zh-hans/%E5%87%B8%E5%87%BD%E6%95%B0\" target=\"_blank\" rel=\"noopener\">凸函数（Convex）</a>，即他们没有局部最小值，只有一个最优解。</p>\n<p>我们稍微看一眼做梯度下降时，实际微分出来的式子，并不关键，看着烦可以忽略…</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/za88j.png\" alt data-zoomable></p></div>\n<p>总之，我们得以用数学上的梯度下降来找到最佳的方法（Function）了。</p>\n<h2 id=\"善后处理\"><a class=\"markdownIt-Anchor\" href=\"#善后处理\"></a> 善后处理<a href=\"post/regression#善后处理\"></a></h2>\n<h3 id=\"验证得到的方法\"><a class=\"markdownIt-Anchor\" href=\"#验证得到的方法\"></a> 验证得到的方法<a href=\"post/regression#验证得到的方法\"></a></h3>\n<p>根据我们最初设定的模型（Model），我们已经得到了最好的 w 和 b 了。那么实际结果怎么样呢？接下来我们就要开始用真实数据对照来看这个方法具体表现的好坏了。</p>\n<p>办法很简单，我们先把得到最好的 w 和 b 带进式子，然后式子作图如下。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/jqbmd.png\" alt data-zoomable></p></div>\n<p>然后我们再把事先收集好的用来训练模型的 10 个真实数据也标到图中。我们发现真实数据跟方法计算出的数据还是有偏差的。我们需要量化一下这个偏差：我们把每个点位方法算出的值和真实数据值取差值然后平均，算出来平均误差有 31.9。</p>\n<p>这还没完，我们真正关心的其实不是训练数据跟我们方法的偏差，而应该关心的是这个方法拿出去给别人用，即碰到了新的输入数据，他输出的跟未知的真实值差别会有多少 ，会不会丢人。</p>\n<p>于是我们又收集到了新的 10 只宝可梦进化前后的数据，如下图。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/s27gb.png\" alt data-zoomable></p></div>\n<p>我们还按照上面的误差量化方式，算出平均误差有 35.0，比训练数据上表现还差一点（这其实很好理解，因为我们完全就是找了个方法来尽量贴合训练数据，显然训练数据上他的贴合程度会更好…嗯这话好绕）。</p>\n<p>35.0 其实误差我们觉得有点大了，有办法改善吗？</p>\n<p>有一种想法是：<a href=\"https://www.nintendo.com/\" target=\"_blank\" rel=\"noopener\">任天堂</a>应该不会只用这么个简单的一次式子来计算进化后的战斗值吧，如果用更复杂的式子会怎么样？比如二次方式子。</p>\n<p>于是我们把模型更新为二次方式子（依然假设输入特性只有一个「进化前战斗值」），如图。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/o03oy.png\" alt data-zoomable></p></div>\n<p>我们按照刚才梯度下降的做法，最终得到最优的 w（因为二次方，所以有两个）和 b。带入算一下平均误差，哇果然小了！</p>\n<p>那是不是有可能三次方式子会更好？我们再把模型换成三次方式子，尝试如下。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/go34p.png\" alt data-zoomable></p></div>\n<p>嗯，好像只好了一点点，那我们再试试四次方式子？</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/adxe9.png\" alt data-zoomable></p></div>\n<p>发现训练数据的误差更小了，但是测试数据的误差反而差了！</p>\n<p>我们再顺手看一眼五次方式子时候会怎样吧。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/0u038.png\" alt data-zoomable></p></div>\n<p>五次方的这个图看起来就很荒唐了，500 左右进化前战斗值的神奇宝贝，进化后战斗值负了？这不对了。所以五次方模型时候的测试数据误差爆炸了。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/mrzh8.png\" alt data-zoomable></p></div>\n<p>于是综上来看，我们选的模型越复杂，参数越多（当然要确保这个式子能解），我们就可以让最终得到的式子和训练数据的误差越低。这其实换个角度也很好理解，越复杂越高次方的式子，他覆盖的模型就越多，理论上可以越能贴合训练数据。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/r84v3.png\" alt data-zoomable></p></div>\n<p>但是越复杂的方法可能在训练数据上能有好结果，但在测试数据上就不一定了…如下图，超过四次方的时候，错误就爆炸了。这种训练数据上表现很好，测试数据表现不好的现象，我们称之为「过度拟合（Overfitting）」。</p>\n<p>具体的原因我们以后会解释清楚，但道理就很像我们在驾校考科目二的时候，我们会背类似「后视镜越过那个电线杆上的贴纸后，方向左转半圈」这样的口诀，而且往往在科目二时候还很有用。但是到了真实驾驶时候，这个就不起作用了。</p>\n<p>今天做到了这里，如果我们要结束开始选最终方法（Function）了，我们就应该选使得我们错误率最低的三次方的式子了。但故事还没结束，我们收集的 10 个数据其实还太少，如果我们收集到 60 个数据的时候，我们又发现了新的事情…</p>\n<h3 id=\"引入更多参数\"><a class=\"markdownIt-Anchor\" href=\"#引入更多参数\"></a> 引入更多参数<a href=\"post/regression#引入更多参数\"></a></h3>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/gqz4t.png\" alt data-zoomable></p></div>\n<p>卧槽它竟然有分叉…原来真实的宝可梦世界里，人家是按照物种分类来区分计算进化后战斗值的…</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/h9son.png\" alt data-zoomable></p></div>\n<p>那么如果我们已经知道了这个事情，要再改进的话，我们就应该分物种来考虑了…</p>\n<p>于是我们加入一个 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mi>s</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">s</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的参数，来扩充刚才那个比较简单的模型，如下图。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/rnvm3.png\" alt data-zoomable></p></div>\n<p>这好像不算线性模型了，有竟然有「if（编码中的条件判断逻辑，如果）」这样的东西，于是我们可以在数学上做如下图的转换，让他可以微分。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/sv1pn.png\" alt data-zoomable></p></div>\n<p>这个式子其实不用管它很复杂的部分，道理是简单的，其中 δ 实际发挥了「if」的作用。比如当物种是波波的时候，只有前面两项是存在的，如下图。左下篮框中的部分可以视为是一个变化的参数 x，故可以将其视为线性模型。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/mh95o.png\" alt data-zoomable></p></div>\n<p>实际按照上面一样的方法，分物种来做模型之后，训练数据上和测试数据上的错误都更小了，分种类处理宝可梦用的参数后，结果果然更好了。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/959jq.png\" alt data-zoomable></p></div>\n<p>那尝到了甜头之后，很自然地我们可能还会想，如果再进一步提高，是不是还可以挖掘出别的因素？可能真的跟进化前宝可梦的体重、身高、体力值等等都有关（这些因素对进化后的战斗值作图如下）？那我们都加上试试看吧！</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/l547u.png\" alt data-zoomable></p></div>\n<p>我们为了简单一些，取二次方的式子做模型的一部分 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>y</mi><mo mathvariant=\"normal\">′</mo></msup></mrow><annotation encoding=\"application/x-tex\">y&#x27;</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.946332em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span>，然后另一部分加上身高、体重、体力值三个参数如图。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/v9kko.png\" alt data-zoomable></p></div>\n<p>结果似乎可以预料（参数那么多…），很低的训练数据错误，但过度拟合了。</p>\n<h3 id=\"想办法调整一下\"><a class=\"markdownIt-Anchor\" href=\"#想办法调整一下\"></a> 想办法调整一下<a href=\"post/regression#想办法调整一下\"></a></h3>\n<p>到此时，如果咱是大木博士<img src=\"http://cdn.1ouo1.com/l4lgx.png\" alt>，那我可以根据脑袋里的知识来删掉一些没用的参数。</p>\n<p>但我是个普通人的话，嗯，也还有一招。我们来重新定义 L 函数如下。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/pri3c.png\" alt data-zoomable></p></div>\n<p>这招叫「修正（Regularization）」。即我们来重新告诉 L 函数，帮我盯着怎么样才叫真的「好方法」，我们会在 L 函数里加上一个额外的项目，如上图。</p>\n<p>其中 λ 是事先手动设置的值。这项加入的作用是，它使得我们同时需要找到一个使参数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">w_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 足够小的方法。</p>\n<p>理由是，参数值 w 越接近于 0，实际上方法的曲线就会越平滑。这样的方法对于输入的 x 值的变化就会比较不敏感，那么其实也就能减少掉噪声数据带来的干扰。</p>\n<p>特别注意，这里不用考虑偏移值 b 的问题，因为 b 只影响到方法的整体偏移而已，跟平滑度无关，事实上不考虑 b 效果还会更好。</p>\n<p>而上面说的 λ 其实就代表了我们要多大程度来关心「平滑度」的这件事。λ 越大，意味着我们会更多考虑去平滑曲线，而更少考虑降低错误了。这其中存在一个平衡关系。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/73hqq.png\" alt data-zoomable></p></div>\n<p>我们改善了 L 函数后，做出结果如上。会发现，当曲线有平滑度考虑加进来后，测试数据的偏差果然小了。但如果曲线太平滑，那其实就接近于方法曲线水平线了（谁都撬动不了他），这样测试数据表现也不好，所以验证了我们说的这需要我们来合理确定 λ 值，找一个最合适的平滑程度。</p>\n<p>所以最终就是这样啦，我们加入了平滑度考虑，在 λ = 100 时，得到最优的方法。</p>\n<p>当然了，如果我们把这个方法直接拿出去交给广大用户来使用，其实得到的错误率还是会比 11.1 要更高的，这个就开始涉及到「验证（Validation）」的知识了，我们下次再聊。</p>\n","prev":{"title":"聊明白机器学习 | 三. 误差都是哪来的？　","slug":"errors"},"next":{"title":"AR、MR 设计资源导航 | v1.0","slug":"ar"},"link":"https://talk2.it/post/regression/","toc":[{"title":"<a class=\"markdownIt-Anchor\"></a> 前言干话","id":"前言干话","index":"1"},{"title":"<a class=\"markdownIt-Anchor\"></a> 拿例子说回归","id":"拿例子说回归","index":"2","children":[{"title":"<a class=\"markdownIt-Anchor\"></a> 我的宝可梦要不要进化？","id":"我的宝可梦要不要进化","index":"2.1"},{"title":"<a class=\"markdownIt-Anchor\"></a> 寻找模型","id":"寻找模型","index":"2.2"},{"title":"<a class=\"markdownIt-Anchor\"></a> 定义方法好坏","id":"定义方法好坏","index":"2.3"},{"title":"<a class=\"markdownIt-Anchor\"></a> 找到最佳参数","id":"找到最佳参数","index":"2.4"}]},{"title":"<a class=\"markdownIt-Anchor\"></a> 善后处理","id":"善后处理","index":"3","children":[{"title":"<a class=\"markdownIt-Anchor\"></a> 验证得到的方法","id":"验证得到的方法","index":"3.1"},{"title":"<a class=\"markdownIt-Anchor\"></a> 引入更多参数","id":"引入更多参数","index":"3.2"},{"title":"<a class=\"markdownIt-Anchor\"></a> 想办法调整一下","id":"想办法调整一下","index":"3.3"}]}],"reward":true,"copyright":{"author":"Bingo","link":"<a href=\"https://talk2.it/post/regression/\" title=\"聊明白机器学习 | 二. 回归（Regression）　\">https://talk2.it/post/regression/</a>","license":"Attribution-NonCommercial-NoDerivatives 4.0 International (<a href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\" rel=\"external nofollow noopener\" target=\"_blank\">CC BY-NC-ND 4.0</a>)"}}