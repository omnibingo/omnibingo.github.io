{"title":"聊明白机器学习 | 三. 误差都是哪来的？　","date":"2019-05-26T12:20:24.000Z","thumbnail":"http://cdn.1ouo1.com/zc9cd.jpg","slug":"errors","comments":true,"tags":["Basic"],"updated":"2019-06-02T04:23:11.000Z","content":"<h2 id=\"前言干话\"><a class=\"markdownIt-Anchor\" href=\"#前言干话\"></a> 前言干话<a href=\"post/errors#前言干话\"></a></h2>\n<p>换工作的过渡期，真一点不比原来工作时间轻松啊。有太多上家的牵挂（当然最终人家可能不这么看你），和默默给下家准备的惊喜都需要十分用心。</p>\n<p>最近也开始重启减重计划了，最近两天直降 5 斤。我基数比较大，方法也比较笨，无非少吃糖脂多流汗。一开始可能会有反复时间，管住嘴憋得很难受，结果发现体重竟然还纹丝不动。但所幸稍稍挺住这个时间会比较快过去，接下来会进入对食物的低欲望周期，就是吃点啥都无所谓了，此时就可以正常上你的减脂食谱了。同时锻炼后的身体肌肉此时会不断给你带来良好的正向反馈，美滋滋。夏天结束后，咱们再来看成果。<br>\n最近还弄了个方便大家听逼哥 2019 跨年音乐的小功能，点击左面板上的 <img src=\"http://cdn.1ouo1.com/idurr.png\" alt> 即可使用。</p>\n<p>下面进入正文，今天细说一下回归模型中的误差问题。</p>\n<a id=\"more\"></a>\n<h2 id=\"误差的两个来源\"><a class=\"markdownIt-Anchor\" href=\"#误差的两个来源\"></a> 误差的两个来源<a href=\"post/errors#误差的两个来源\"></a></h2>\n<p>上一篇里面我们说到了训练时使用不同的模型（Model）中，可能会出现不同的误差情况。甚至有些时候越复杂的模型可能带来越差的表现。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/q8ibx.png\" alt data-zoomable></p></div>\n<p>比如上图，横坐标是模型的次方数，纵坐标是误差值。后面模型复杂到四次方、五次方时，错误率就起飞了。</p>\n<p>今天我们就来细看一下这个误差。开门见山来说，这个误差其实有两个来源，分别是：偏离值（Bias）和方差（Variance）。我们后面就想办法针对这两方面误差，使用恰当的办法来提升模型的表现。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/4chhu.png\" alt data-zoomable></p></div>\n<p>回到上一次宝可梦的例子里。我们算一直杰尼龟进化后的战斗值（CP），真实的算法 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>f</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{f}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1523199999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9578799999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.08332999999999999em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> 其实只有宝可梦的制作公司（Niantic）知道，我们平民百姓只能根据训练数据来找到最为接近它的方法 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo>∗</mo></mrow><annotation encoding=\"application/x-tex\">{f}*</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span><span class=\"mord\">∗</span></span></span></span>，相当于找这个真实算法的估测值。</p>\n<p>这件事情就好像我们确立了一个靶子，理想靶心的位置就是 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>f</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{f}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1523199999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9578799999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.08332999999999999em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span>，我们最终得到的平民答案 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo>∗</mo></mrow><annotation encoding=\"application/x-tex\">{f}*</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span><span class=\"mord\">∗</span></span></span></span> 总是和它中间有段距离，这个距离可能来自偏离值（Bias），也可能来自方差（Variance）。</p>\n<h3 id=\"数学上的道理\"><a class=\"markdownIt-Anchor\" href=\"#数学上的道理\"></a> 数学上的道理<a href=\"post/errors#数学上的道理\"></a></h3>\n<p>概率论里我们也遇到过类似的场景。假设有变量 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span></span></span></span>，它的平均值是 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">μ</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>，方差是 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\sigma^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span>（这些拉丁字母的数学含义忘了就忘了，别紧张，领会意思即可）。</p>\n<p>如果我们要估测平均值 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>，正常做法就是我们能去抽样 N 个点，求个平均值即可。但我们心里清楚，这个算得的平均值跟真实的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span> 不一样，会有误差（抽样嘛）。即便我们做了很多次实验得到很多个平均值 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>m</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">m_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">m</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>、<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>m</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">m_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">m</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>、<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>m</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">m_3</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">m</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">3</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>…它们都很难正好等于 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>。但我们知道如果要说平均值 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">m</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">m</span></span></span></span> 的期望（Expect）值 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">E(m)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">m</span><span class=\"mclose\">)</span></span></span></span>，那它确实是正好等于 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span> 的，这应该好理解。</p>\n<p>特别地，在上面概率论这个场景里，我们用 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">m</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">m</span></span></span></span> 去估测平均值时候，因为我们明确知道「靶心」要瞄准的值为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>，故此时这是个没有偏离值（Bias）的情况，后面会提到不一样的情况。</p>\n<p>做概率论例子估测的实操时，我们将取很多样本值（特别注意，下图素材中的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>x</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">x^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span> 代表第二组 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span></span></span></span> 数据，如果具体表示第二组 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span></span></span></span> 中的第一、第二个值，则表示为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>x</mi><mn>1</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">x^2_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0622159999999998em;vertical-align:-0.24810799999999997em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-2.4518920000000004em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.24810799999999997em;\"><span></span></span></span></span></span></span></span></span></span>、<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>x</mi><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">x^2_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0622159999999998em;vertical-align:-0.24810799999999997em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-2.4518920000000004em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.24810799999999997em;\"><span></span></span></span></span></span></span></span></span></span>，以后也会有类似编号写在上标位置的情况，不要误会成次方了），接着我们算出平均值 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">m</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">m</span></span></span></span>，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">m</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">m</span></span></span></span> 的期望值是 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>，且它没有偏离值（Unbiased）。则实际样本就是如图均匀散落在 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span> 周围的，但很难直接等于 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/snc4z.png\" alt data-zoomable></p></div>\n<p>那么会散落得多分散呢？这就取决于估测平均值 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">m</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">m</span></span></span></span> 的方差（Variance）了。方差等式如下图，这是概率论知识，了解意思即可。<strong>这个方差值越大，样本越分散，反之越集中</strong>。方差的大小，跟每次抽样我们「拿多少样本作为一份」有关。道理很简单：我们每次抽样时候多拿一些样本数据来算平均，自然它就更精准，每次算得的结果 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">m</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">m</span></span></span></span> 就更接近真实平均值 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/qzbet.png\" alt data-zoomable></p></div>\n<p>这个方差我们当然也要估测。我们一般会用下图中这个带 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>s</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">s^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">s</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span> 的式子来估测方差 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup><mi mathvariant=\"normal\">/</mi><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma^2/N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.064108em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mord\">/</span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 。但注意，此时的估测值就是有偏离的了，在数学上 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>s</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">s^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">s</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span> 会比 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\sigma^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span> 稍小，注意有个 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo>−</mo><mn>1</mn><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">/</mi><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">(N-1)/N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">)</span><span class=\"mord\">/</span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span></span></span></span>。所以从图上的串串来看，发现蓝色的估测点虽然也是落在真正的方差周围，但比它小的点更多。并且当我们扩大每次的抽样量 N，得到的估测点就会更集中分布在真正的方差周围，即上面提到的那个道理。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/t6uza.png\" alt data-zoomable></p></div>\n<h3 id=\"这俩问题的综述\"><a class=\"markdownIt-Anchor\" href=\"#这俩问题的综述\"></a> 这俩问题的综述<a href=\"post/errors#这俩问题的综述\"></a></h3>\n<p>所以回归到靶子的例子来更形象地总结一下这两方面问题。</p>\n<p>我们要估测靶心目标，拿了很多样本，我们辛辛苦苦找了每一组样本对应的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>f</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">f^*</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.688696em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mbin mtight\">∗</span></span></span></span></span></span></span></span></span></span></span>，把这些 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>f</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">f^*</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.688696em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mbin mtight\">∗</span></span></span></span></span></span></span></span></span></span></span> 取平均，得到 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\bar{f}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0256599999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8312199999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.08332999999999999em;\">ˉ</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span>，它就是 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>f</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">f^*</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.688696em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mbin mtight\">∗</span></span></span></span></span></span></span></span></span></span></span> 的期望值了，换句话说就是我们认为的靶心。那么这个<strong>靶心（我们每次心里想瞄准的地方） <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\bar{f}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0256599999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8312199999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.08332999999999999em;\">ˉ</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> 和真实的靶心 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>f</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{f}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1523199999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9578799999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.08332999999999999em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> 之间就存在偏差，这个偏差就叫偏离值（Bias）</strong>。</p>\n<p>此外，每次我们瞄准了，但手不听使唤容易打不准，故每次「射击」都还存在一个<strong>瞄准位置和实际射中位置的偏差，这个偏差就叫方差（Variance）</strong>。下图中对应了两个偏差出现的场景。我们最终追求的是左上角的情况，即没有偏离值，且方差最小。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/x67em.png\" alt data-zoomable></p></div>\n<h2 id=\"问题与模型的关系\"><a class=\"markdownIt-Anchor\" href=\"#问题与模型的关系\"></a> 问题与模型的关系<a href=\"post/errors#问题与模型的关系\"></a></h2>\n<p>前面说到，为了更精确地估测真正的方法 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>f</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{f}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1523199999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9578799999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.08332999999999999em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span>，我们需要通过多次试验来找到多个 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>f</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">f^*</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.688696em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mbin mtight\">∗</span></span></span></span></span></span></span></span></span></span></span>，这里为了节约篇幅，不具体讲某一次的试验，而是我们通过更高的视角来看这件「不断靠近真实 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>f</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{f}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1523199999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9578799999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.08332999999999999em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> 的」事情。</p>\n<p>我们进行多次试验的过程中，每一次都会采集不同的样本，每一次也都会产生出不同的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>f</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">f^*</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.688696em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mbin mtight\">∗</span></span></span></span></span></span></span></span></span></span></span>。科幻点讲，这就很像我们在很多个平行宇宙（Parallel Universes）里为了同一个目标（正确估测进化后战斗值），分别抓 10 只不同的宝可梦，分别做一次试验的感觉。如下图。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/co6cg.png\" alt data-zoomable></p></div>\n<p>很自然，对于每个「平行宇宙」，我们给不同的数据，找出来的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>f</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">f^*</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.688696em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mbin mtight\">∗</span></span></span></span></span></span></span></span></span></span></span> 肯定不一样嘛。比如 123 号平行宇宙里，我们找出来的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>f</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">f^*</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.688696em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mbin mtight\">∗</span></span></span></span></span></span></span></span></span></span></span> 如下图左边；而 345 号平行宇宙里，我们找出来的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>f</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">f^*</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.688696em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mbin mtight\">∗</span></span></span></span></span></span></span></span></span></span></span> 如下图右边。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/nxtcw.png\" alt data-zoomable></p></div>\n<h3 id=\"平行宇宙里的方差\"><a class=\"markdownIt-Anchor\" href=\"#平行宇宙里的方差\"></a> 平行宇宙里的方差<a href=\"post/errors#平行宇宙里的方差\"></a></h3>\n<p>每个平行宇宙的一次实验就像是我们对着靶开出不同的每一枪一样。</p>\n<p>假如有 100 个平行宇宙（好富有的感觉），那就像是开出了 100 枪。我们把它们每次做出的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>f</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">f^*</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.688696em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mbin mtight\">∗</span></span></span></span></span></span></span></span></span></span></span> 分别画出图如下。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/c7pja.png\" alt data-zoomable></p></div>\n<p>注意看，图里实际上画了三种不同复杂程度的模型（Model）分别操作是什么情况。其中五次方的复杂模型看起来挺恐怖的，好像这里面中每一个值都会被触发一样。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/3mzyg.png\" alt data-zoomable></p></div>\n<p>所以从图里来看，<strong>模型如果比较简单，得到的结果反倒是集中的，即方差（Variance）小</strong>，简单来理解就是这个模型的输出结果受样本数据的影响比较小（试想一下极限情况：最简单的模型即一条常量的直线 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">f(x)=c</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">c</span></span></span></span>，此时方差为 0，它不受样本的任何影响，就一根线，无比集中）；反之，模型如果比较复杂，散布就会很大很开，每条线的差别都很大，即方差（Variance）很大。</p>\n<h3 id=\"平行宇宙里的偏离值\"><a class=\"markdownIt-Anchor\" href=\"#平行宇宙里的偏离值\"></a> 平行宇宙里的偏离值<a href=\"post/errors#平行宇宙里的偏离值\"></a></h3>\n<p>至于偏离值（Bias），我们前面说过，指的是我们已经有了多个 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>f</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">f^*</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.688696em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mbin mtight\">∗</span></span></span></span></span></span></span></span></span></span></span>，得到期望值 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\bar{f}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0256599999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8312199999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.08332999999999999em;\">ˉ</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> 后，这个 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\bar{f}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0256599999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8312199999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.08332999999999999em;\">ˉ</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> 与真实的方法 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>f</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{f}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1523199999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9578799999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.08332999999999999em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> 之间的距离，它就是偏离值（Bias）。</p>\n<p>为了感受偏离值和不同模型之间的关联关系，我们假定下图里的黑线是我们孜孜追求的真实方法 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>f</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{f}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1523199999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9578799999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.08332999999999999em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> 的样子。那么意味着宝可梦的进化后战斗值就是从这个黑线上算来的，也即我们每次做实验抽的 10 个点，都是拿的这黑线上的某 10 个点。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/omn4z.png\" alt data-zoomable></p></div>\n<p>那么接下来我们再来图里看，黑线是真实方法，红线是我们通过 5000（为了突出效果我们把试验次数提高到 5000 ）次试验每次得到的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>f</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">f^*</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.688696em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mbin mtight\">∗</span></span></span></span></span></span></span></span></span></span></span>，蓝线是我们拿所有红线求均值得到的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\bar{f}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0256599999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8312199999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.08332999999999999em;\">ˉ</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span>。</p>\n<p>同理，为了感受模型复杂度与偏离值的关系，上面三个图分别是一次方模型、三次方模型、五次方模型分别操作得到的图。</p>\n<p>我们会发现，虽然越高次越复杂得到的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>f</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">f^*</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.688696em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mbin mtight\">∗</span></span></span></span></span></span></span></span></span></span></span> 画图看起来很可怕，都铺满了，每个 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>f</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">f^*</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.688696em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mbin mtight\">∗</span></span></span></span></span></span></span></span></span></span></span> 相差很多，但平均起来，反而是最接近真实方法 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>f</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{f}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1523199999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9578799999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.08332999999999999em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> 的，即：<strong>越复杂的模型，会拿到越小的偏离值（Bias）。</strong></p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/lj8gl.png\" alt data-zoomable></p></div>\n<p>这件事可以这样理解：越高次方越复杂的模型，他所覆盖的可能性空间就越大，包含你理想方法的可能性也就越大，你只要找出来，它就能给你偏差很小的结果。那么<strong>反之，越简单的模型，会有越小的方差（Variance），有越大的偏离值（Bias）</strong>。</p>\n<h3 id=\"一个平衡问题\"><a class=\"markdownIt-Anchor\" href=\"#一个平衡问题\"></a> 一个平衡问题<a href=\"post/errors#一个平衡问题\"></a></h3>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/dl3vc.png\" alt data-zoomable></p></div>\n<p>所以我们可以把模型选择和误差表现的关系作图如上。从左到右，随着模型越来越复杂，偏离值（Bias）下降，方差（Variance）上升。</p>\n<p>于是乎，这又回到了一个平衡问题了。图中的蓝线是一个综合总体误差水平，我们要综合考虑偏离值和方差，找一个使得综合总体误差最小的模型。</p>\n<p>特别地，我们把偏离值（Bias）大的错误情况叫做「欠拟合（Underfitting）」，把方差（Variance）大的错误情况叫做「过拟合（Overfitting）」，过拟合也有时被称作「过度拟合」。</p>\n<h2 id=\"具体实操\"><a class=\"markdownIt-Anchor\" href=\"#具体实操\"></a> 具体实操<a href=\"post/errors#具体实操\"></a></h2>\n<p>当我们拿到一个待优化的模型，我们如何去分辨这个模型的问题主要在偏离值（Bias）上还是在方差（Variance）上呢？</p>\n<h3 id=\"偏离值bias大咋办\"><a class=\"markdownIt-Anchor\" href=\"#偏离值bias大咋办\"></a> 偏离值（Bias）大咋办<a href=\"post/errors#偏离值bias大咋办\"></a></h3>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/rxqur.png\" alt data-zoomable></p></div>\n<p>我们可以去看实验中的现象。如果你这个模型连「训练用的真实样本数据」都无法很好地吻合上，那说明模型离正确的模型还有很大的距离，即偏离值（Bias）很大，你欠拟合（Underfitting）了。</p>\n<p>如果偏离值（Bias）大，说明你追求的真实方法 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>f</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{f}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1523199999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9578799999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.08332999999999999em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> 压根不在你的模型覆盖圈里，这时就需要重新设计这个模型了。比如找更多的特性来扩充丰富模型（宝可梦的血量、体重啊之类），或者让模型更复杂（选用更多次方的式子之类）。特别注意，这时候我们去找更多的训练数据是没用的。</p>\n<h3 id=\"方差variance大咋办\"><a class=\"markdownIt-Anchor\" href=\"#方差variance大咋办\"></a> 方差（Variance）大咋办<a href=\"post/errors#方差variance大咋办\"></a></h3>\n<p>反过来，如果你这个模型能很好地吻合样本数据，但是一跑测试数据就表现不好，则是方差（Variance）很大，你过度拟合（Overfitting）了。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/n2252.png\" alt data-zoomable></p></div>\n<p>如果方差（Variance）大，一个很有效的办法就是：去更多地搜集训练数据！比如上图，如果每次我们能从 10 个样例数据扩展到 100 个样例数据来训练，得到的结果会集中很多。一句话就是：<strong>这时候，数据多真的是可以为所欲为的！</strong></p>\n<p>当然了，想想也知道：总是要更多数据往往不太现实（甚至让你可能容易被炒鱿鱼），此时，聪明的我们还留有两招可用。</p>\n<p>第一招，是按照你的理解，去自己制作更多数据用于训练。比如做图像识别训练，我们把图像转个角度，就得到新的图片（因为你训练的机器回头也得能识别不同角度的东西嘛）；再比如声音识别训练，我们用变声器处理一下已有的声音，得到不同性别的声音…或者添加背景噪音，去让模型<a href=\"https://zh.wikipedia.org/wiki/%E9%B2%81%E6%A3%92%E6%80%A7\" target=\"_blank\" rel=\"noopener\">健壮性</a>更强；再比如多语种语义识别训练，我们把已有语义批量翻译成其他语种…</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/jgwmz.png\" alt data-zoomable></p></div>\n<p>第二招，是我们在模型里添加一项调整（Regularization）项，这个跟我们<a href=\"http://talk2.it/post/regression#%E6%83%B3%E5%8A%9E%E6%B3%95%E8%B0%83%E6%95%B4%E4%B8%80%E4%B8%8B\">前一篇</a>里增加调整项的思路异曲同工。具体做法是，我们在 L 函数（损失函数，Loss Function）中增加一项，来约束参与的参数，让他们尽可能小，以使得得到的曲线尽可能平滑。当然，我们同样还需要一个 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">λ</span></span></span></span> 值来控制我们多大程度地考虑平滑度问题。平滑后效果如下图。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/a71fu.png\" alt data-zoomable></p></div>\n<p>但要注意的是，第二招这个操作可能会使得偏离值（Bias）有所增加。因为你过滤掉了一些不那么平滑度的函数，这些函数里是可能包含有更好的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span> 的。</p>\n<h3 id=\"选模型时候要小心\"><a class=\"markdownIt-Anchor\" href=\"#选模型时候要小心\"></a> 选模型时候要小心<a href=\"post/errors#选模型时候要小心\"></a></h3>\n<p>我们得到多个 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span> 之后，开始平衡偏离值（Bias）和方差（Variance），挑选使得测试误差（Testing error）最小的那个。</p>\n<p>这时要特别注意，对于通过训练数据训练出来的模型们，这时你用你拥有的测试数据（Testing data）来挑选出使得测试数据误差最小的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span>，你接着兴冲冲地拿这个 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span> 去公开投放到真实的测试数据当中，它的表现很经常不如你在使用测试数据时的表现。</p>\n<p>因为你有限的测试数据们的分布，是极有可能跟大量真实数据的分布形态存在偏差。这句话的描述可能不够到位，但产生的结果就是：当两套数据来测一个跟真实方法有偏差的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span> 时，两套数据的误差表现很可能是不一样的。所以你通过你的测试数据调出来的方法嘛，可能只在你自己的测试数据里表现是最好的…</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/dxmce.png\" alt data-zoomable></p></div>\n<p>总之在实操时候，直接通过公共的有限测试数据来挑 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span> 是不可取的。因为真实在「生产环境」里考验它的，那些你没见过的数据（Private Set）可能会让它表现差很多。</p>\n<h3 id=\"推荐的做法\"><a class=\"markdownIt-Anchor\" href=\"#推荐的做法\"></a> 推荐的做法<a href=\"post/errors#推荐的做法\"></a></h3>\n<p>这时候推荐的做法是把我们已有的训练数据（Training Data）分成两份。用第一份作为训练组（Training Set）来训练你的模型；再用另一份作为验证组（Validation Set）来选出错误最小，表现最佳的。</p>\n<p>如果你觉得这样训练组的数据太少了，那么可以在第二份验证组数据选出了最好模型之后，再用整个训练数据一起重新训练这个最佳模型一次。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/par9e.png\" alt data-zoomable></p></div>\n<p>此时，在公开的测试数据上去测这个模型，你会发现错误表现比训练时候要更大，但这，才能真实反映在未来公开的，未知的测试数据（Private Set）中你这个模型的错误表现。</p>\n<p>尤其需要<strong>注意避免</strong>的是，你在公开的测试数据测完发现错误率比训练时候高的时候，相信你会很有冲动再回头去调整一下模型，企图让模型在公开测试数据上也表现得更好。这是不可取的（当然你可能经常控制不住自己）！因为这种操作相当于你又加入了公开测试数据集的这些有限数据的偏差影响（跟前面那个不够到位的描述是一个道理）。</p>\n<p>总之，公开的测试数据集上的表现都是「浮云」，真正你需要在乎的是真实投放出去后，在未知的测试数据（Private Testing Set）中的表现。</p>\n<div class=\"article-img\"><p><img src=\"http://cdn.1ouo1.com/wlse6.png\" alt data-zoomable></p></div>\n<p>当然，你如果更考究的话，可以做得更细。比如把训练数据集分成三等分，并且采用不同的切分方案去多次三等分这些数据。每次拆分，里面的两份拿出来作为训练组（Training），剩下一份拿来作为验证组（Validation）。每次拆分完，都把模型丢进去给训练组训练，完后给验证组算错误值。有几套拆分方案就这样操作几次。然后看每个模型的多次验证组错误的平均值，取这个平均错误值最小的模型，作为你的最佳模型。然后拿着他，再经过全体训练数据训练一次后，就可以拿公开测试数据集（Public Testing Set）跑来看一眼了，误差没有特别不可接受的话，就别动人家了。</p>\n<p>总之，原则上，不要太在意这个公开测试数据集（Public Testing Set）上的错误结果，少去根据这个结果来调整模型。往往这样按规矩办事，后面在真实未知数据集（Private Testing Set）上得到的误差才会跟目前实测的误差相差较小。</p>\n<p>好啦，今天先到这，下一篇我们要细说一下梯度下降（Gradient Descent）的操作了，越来越实战，加油。</p>\n","prev":{"title":"产品经理从软到硬，你应该留心的那些事","slug":"hardware"},"next":{"title":"聊明白机器学习 | 二. 回归（Regression）　","slug":"regression"},"link":"https://talk2.it/post/errors/","toc":[{"title":"<a class=\"markdownIt-Anchor\"></a> 前言干话","id":"前言干话","index":"1"},{"title":"<a class=\"markdownIt-Anchor\"></a> 误差的两个来源","id":"误差的两个来源","index":"2","children":[{"title":"<a class=\"markdownIt-Anchor\"></a> 数学上的道理","id":"数学上的道理","index":"2.1"},{"title":"<a class=\"markdownIt-Anchor\"></a> 这俩问题的综述","id":"这俩问题的综述","index":"2.2"}]},{"title":"<a class=\"markdownIt-Anchor\"></a> 问题与模型的关系","id":"问题与模型的关系","index":"3","children":[{"title":"<a class=\"markdownIt-Anchor\"></a> 平行宇宙里的方差","id":"平行宇宙里的方差","index":"3.1"},{"title":"<a class=\"markdownIt-Anchor\"></a> 平行宇宙里的偏离值","id":"平行宇宙里的偏离值","index":"3.2"},{"title":"<a class=\"markdownIt-Anchor\"></a> 一个平衡问题","id":"一个平衡问题","index":"3.3"}]},{"title":"<a class=\"markdownIt-Anchor\"></a> 具体实操","id":"具体实操","index":"4","children":[{"title":"<a class=\"markdownIt-Anchor\"></a> 偏离值（Bias）大咋办","id":"偏离值bias大咋办","index":"4.1"},{"title":"<a class=\"markdownIt-Anchor\"></a> 方差（Variance）大咋办","id":"方差variance大咋办","index":"4.2"},{"title":"<a class=\"markdownIt-Anchor\"></a> 选模型时候要小心","id":"选模型时候要小心","index":"4.3"},{"title":"<a class=\"markdownIt-Anchor\"></a> 推荐的做法","id":"推荐的做法","index":"4.4"}]}],"reward":true,"copyright":{"author":"Bingo","link":"<a href=\"https://talk2.it/post/errors/\" title=\"聊明白机器学习 | 三. 误差都是哪来的？　\">https://talk2.it/post/errors/</a>","license":"Attribution-NonCommercial-NoDerivatives 4.0 International (<a href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\" rel=\"external nofollow noopener\" target=\"_blank\">CC BY-NC-ND 4.0</a>)"}}